---
name: migration-agent
description: "Assists with pipeline migration from Prefect 1.4 to Prefect 3.0. Run only when a migration is requested."
model: haiku
---

# Contexto de MigraÃ§Ã£o: Prefect 1.4 â†’ Prefect 3.0

## RepositÃ³rios
- **Origem (1.4):** /caminho/para/pipelines_rj_smtr (Prefect 1.4)
- **Destino (3.0):** /caminho/para/pipelines_v3 (Prefect 3.0)

---

## âš ï¸ REGRAS IMPORTANTES

### 0. Atualizar RepositÃ³rio de Origem Antes de Migrar

**SEMPRE** atualize a branch main do repositÃ³rio de origem antes de iniciar qualquer migraÃ§Ã£o:
```bash
cd /caminho/para/pipelines_rj_smtr
git checkout main
git pull origin main
```

Isso garante que vocÃª estÃ¡ migrando a versÃ£o mais recente do cÃ³digo.

### 1. Usar Cookiecutter para Criar Novos Pipelines

**SEMPRE** use o cookiecutter para criar a estrutura de um novo pipeline:
```bash
cd /caminho/para/pipelines_v3
uvx cookiecutter templates --output-dir=pipelines
```

Isso garante que todos os arquivos necessÃ¡rios sejam criados de forma padronizada:
- `pyproject.toml`
- `Dockerfile`
- `prefect.yaml`
- `CHANGELOG.md`
- `constants.py`
- `flow.py`
- `.dockerignore`

**Templates disponÃ­veis:**
- `capture` - Para pipelines de captura de dados
- `treatment` - Para pipelines de materializaÃ§Ã£o/tratamento

### 2. Verificar `pipelines/common/` Antes de Criar CÃ³digo Novo

**ANTES** de criar qualquer task ou funÃ§Ã£o nova, **SEMPRE** verifique se jÃ¡ existe algo similar em:
```
pipelines/common/
â”œâ”€â”€ capture/
â”‚   â”œâ”€â”€ default_capture/    # Flow genÃ©rico de captura
â”‚   â”‚   â”œâ”€â”€ flow.py         # create_capture_flows_default_tasks()
â”‚   â”‚   â”œâ”€â”€ tasks.py        # Tasks reutilizÃ¡veis de captura
â”‚   â”‚   â””â”€â”€ utils.py        # SourceCaptureContext, helpers
â”‚   â””â”€â”€ jae/                # EspecÃ­fico da JaÃ© (exemplo)
â”‚       â”œâ”€â”€ constants.py
â”‚       â”œâ”€â”€ tasks.py
â”‚       â””â”€â”€ utils.py
â”œâ”€â”€ treatment/
â”‚   â””â”€â”€ default_treatment/  # Flow genÃ©rico de materializaÃ§Ã£o
â”‚       â”œâ”€â”€ flow.py         # create_materialization_flows_default_tasks()
â”‚       â”œâ”€â”€ tasks.py        # Tasks reutilizÃ¡veis de tratamento
â”‚       â””â”€â”€ utils.py        # DBTSelector, DBTTest, helpers
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ gcp/
â”‚   â”‚   â”œâ”€â”€ bigquery.py     # SourceTable, Dataset, BQTable
â”‚   â”‚   â””â”€â”€ storage.py      # Storage
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â””â”€â”€ db.py           # get_raw_db, get_raw_db_paginated
â”‚   â”œâ”€â”€ database.py         # create_database_url, test_database_connection
â”‚   â”œâ”€â”€ discord.py          # send_discord_message
â”‚   â”œâ”€â”€ fs.py               # save_local_file, read_raw_data
â”‚   â”œâ”€â”€ pretreatment.py     # transform_to_nested_structure
â”‚   â”œâ”€â”€ redis.py            # get_redis_client
â”‚   â”œâ”€â”€ secret.py           # get_secret
â”‚   â””â”€â”€ utils.py            # convert_timezone, cron helpers
â”œâ”€â”€ constants.py            # Constantes globais (TIMEZONE, PROJECT_NAME, etc)
â””â”€â”€ tasks.py                # Tasks genÃ©ricas (get_run_env, get_scheduled_timestamp)
```

**Ordem de verificaÃ§Ã£o:**
1. `pipelines/common/tasks.py` - Tasks genÃ©ricas
2. `pipelines/common/capture/default_capture/` - Para flows de captura
3. `pipelines/common/treatment/default_treatment/` - Para flows de tratamento
4. `pipelines/common/utils/` - FunÃ§Ãµes utilitÃ¡rias
5. `pipelines/common/capture/{fonte}/` - CÃ³digo especÃ­fico da fonte (ex: jae)

**Se encontrar algo similar:** Reutilize ou estenda
**Se nÃ£o encontrar:** Crie em `common/` se for reutilizÃ¡vel, ou no pipeline especÃ­fico se for Ãºnico

### 3. Copiar Modelos DBT para Flows de Treatment

**Para flows de treatment**, alÃ©m de migrar o cÃ³digo Python, Ã© necessÃ¡rio copiar os modelos DBT correspondentes ao selector utilizado, incluindo modelos de staging, snapshots e documentaÃ§Ã£o.

**Estrutura de pastas DBT:**
```
# RepositÃ³rio origem (1.4)
pipelines_rj_smtr/queries/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ {dominio}/              # Ex: cadastro, transito, bilhetagem
â”‚       â”œâ”€â”€ staging/            # Modelos de staging (transformaÃ§Ãµes intermediÃ¡rias)
â”‚       â”‚   â”œâ”€â”€ stg_*.sql       # Modelos prefixados com stg_
â”‚       â”‚   â””â”€â”€ aux_*.sql       # Modelos auxiliares
â”‚       â”œâ”€â”€ *.sql               # Modelos finais
â”‚       â”œâ”€â”€ schema.yml          # DocumentaÃ§Ã£o e testes dos modelos
â”‚       â””â”€â”€ CHANGELOG.md        # HistÃ³rico de mudanÃ§as
â””â”€â”€ snapshots/
    â””â”€â”€ {dominio}/
        â””â”€â”€ snapshot_*.sql      # Snapshots para rastreamento histÃ³rico

# RepositÃ³rio destino (3.0)
pipelines_v3/queries/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ {dominio}/              # Mesma estrutura
â””â”€â”€ snapshots/
    â””â”€â”€ {dominio}/              # Mesma estrutura
```

**Passo a passo:**

1. **Identifique o selector** usado no flow de treatment:
```python
   # No arquivo constants.py do flow antigo
   CADASTRO_SELECTOR = DBTSelector(
       name="cadastro",  # <-- Este Ã© o nome do selector
       ...
   )

   # Se houver snapshot, tambÃ©m estarÃ¡ aqui:
   SNAPSHOT_CADASTRO_SELECTOR = DBTSelector(
       name="snapshot_cadastro",
       ...
   )
```

2. **Localize o selector no arquivo de selectors do DBT:**
```bash
   # RepositÃ³rio origem
   cat pipelines_rj_smtr/queries/selectors.yml | grep -A 10 "name: cadastro"
```

   Exemplo de selector:
```yaml
   selectors:
     - name: cadastro
       definition:
         method: fqn
         value: "cadastro"  # Pasta/modelo a ser copiado

     - name: snapshot_cadastro
       definition:
         method: fqn
         value: "snapshot_cadastro"  # Snapshot tambÃ©m pode ter selector
```

3. **Copie a pasta completa de modelos:**
```bash
   # Copiar pasta inteira de modelos (inclui staging/, schema.yml, CHANGELOG.md, etc)
   cp -r pipelines_rj_smtr/queries/models/cadastro/ pipelines_v3/queries/models/
```

   Isso inclui automaticamente:
   - Modelos finais (*.sql)
   - Modelos de staging (staging/*.sql)
   - Modelos auxiliares (aux_*.sql)
   - DocumentaÃ§Ã£o (schema.yml) com testes e descriÃ§Ãµes
   - HistÃ³rico (CHANGELOG.md)

4. **Copie os snapshots relacionados (se existirem):**
```bash
   # Verificar se hÃ¡ snapshots para este domÃ­nio
   ls -la pipelines_rj_smtr/queries/snapshots/{dominio}/

   # Copiar snapshots (crie o diretÃ³rio se nÃ£o existir)
   mkdir -p pipelines_v3/queries/snapshots/{dominio}/
   cp pipelines_rj_smtr/queries/snapshots/{dominio}/*.sql pipelines_v3/queries/snapshots/{dominio}/

   # Exemplo com transito:
   mkdir -p pipelines_v3/queries/snapshots/transito/
   cp pipelines_rj_smtr/queries/snapshots/transito/*.sql pipelines_v3/queries/snapshots/transito/
```

5. **Verifique se os sources estÃ£o definidos:**
```bash
   # Identificar sources usados nos modelos
   grep -roh "source('[^']*', '[^']*')" pipelines_v3/queries/models/cadastro/ | sort -u

   # Verificar se estÃ£o em pipelines_v3/queries/models/sources.yml
   grep "name: {source_name}" pipelines_v3/queries/models/sources.yml

   # Se faltar, copiar do repositÃ³rio origem
   # Isso geralmente jÃ¡ estÃ¡ no destino para os domÃ­nios principais
```

6. **Verifique o arquivo `selectors.yml` do repositÃ³rio destino:**
```bash
   # Verificar se o selector jÃ¡ existe
   grep "name: cadastro" pipelines_v3/queries/selectors.yml

   # Se nÃ£o existir, copiar a definiÃ§Ã£o completa do selector
   # do arquivo de origem para o de destino
```

7. **Verifique dependÃªncias entre modelos:**
```bash
   # Listar refs usadas nos modelos copiados
   grep -roh "ref('[^']*')" pipelines_v3/queries/models/cadastro/ | sort -u

   # Se houver refs para modelos em outros domÃ­nios, verificar se jÃ¡ existem
   # Se nÃ£o existirem, copiar tambÃ©m esses modelos dependentes
```

**Exemplo completo para migrar `treatment__transito_autuacao`:**
```bash
# 1. Atualizar repo origem
cd /caminho/para/pipelines_rj_smtr
git checkout main && git pull origin main

# 2. Copiar modelos (inclui staging/, schema.yml, CHANGELOG.md)
cp -r queries/models/transito/ /caminho/para/pipelines_v3/queries/models/

# 3. Copiar snapshots (se existirem)
mkdir -p /caminho/para/pipelines_v3/queries/snapshots/transito/
cp queries/snapshots/transito/*.sql /caminho/para/pipelines_v3/queries/snapshots/transito/

# 4. Verificar sources usados
grep -roh "source('[^']*', '[^']*')" /caminho/para/pipelines_v3/queries/models/transito/ | sort -u
# Resultado: source('transito_staging', 'infracoes_renainf'), etc

# 5. Verificar se sources existem no destino
grep -A 5 "name: transito_staging" /caminho/para/pipelines_v3/queries/models/sources.yml

# 6. Verificar selectors no destino
grep -A 5 "name: transito_autuacao" /caminho/para/pipelines_v3/queries/selectors.yml
grep -A 5 "name: snapshot_transito" /caminho/para/pipelines_v3/queries/selectors.yml

# 7. Verificar refs entre modelos
grep -roh "ref('[^']*')" /caminho/para/pipelines_v3/queries/models/transito/ | sort -u
# Se houver refs para modelos em outros domÃ­nios, copiar tambÃ©m

# 8. Testar compilaÃ§Ã£o dos modelos
cd /caminho/para/pipelines_v3/queries
dbt parse --select transito  # Verifica se os modelos estÃ£o corretos
```

---

## ğŸ¯ ObservaÃ§Ãµes Importantes da PrÃ¡tica

### DBT Models - O que Copiar

Ao copiar modelos DBT, **sempre copie a pasta inteira** do domÃ­nio, nÃ£o apenas arquivos individuais:

```bash
# âœ“ CORRETO: Copia tudo de uma vez
cp -r pipelines_rj_smtr/queries/models/transito/ pipelines_v3/queries/models/

# âœ— ERRADO: Copia apenas alguns arquivos
cp pipelines_rj_smtr/queries/models/transito/*.sql pipelines_v3/queries/models/transito/
```

**Por quÃª?** Porque a pasta inclui:
- SubdiretÃ³rios (staging/, etc)
- schema.yml (testes e documentaÃ§Ã£o)
- CHANGELOG.md (histÃ³rico)
- Todos os arquivos SQL na estrutura correta

### Snapshots

Snapshots sÃ£o **frequentemente esquecidos** mas essenciais para rastreamento histÃ³rico:

```bash
# Sempre verificar e copiar snapshots
ls pipelines_rj_smtr/queries/snapshots/
# Se existir um diretÃ³rio com o domÃ­nio, copiar
mkdir -p pipelines_v3/queries/snapshots/transito/
cp pipelines_rj_smtr/queries/snapshots/transito/*.sql pipelines_v3/queries/snapshots/transito/
```

### Sources, Selectors e Testes

**Antes de criar um novo cÃ³digo Python**, sempre verifique:

1. **Sources.yml:** Geralmente os sources jÃ¡ existem no destino para domÃ­nios principais
   ```bash
   grep -A 5 "name: transito_staging" pipelines_v3/queries/models/sources.yml
   ```

2. **Selectors.yml:** Pode ser que o selector jÃ¡ exista
   ```bash
   grep "name: transito_autuacao" pipelines_v3/queries/selectors.yml
   ```

3. **Schema.yml:** ContÃ©m testes importantes que jÃ¡ vÃªm com a cÃ³pia dos modelos
   ```bash
   grep -A 10 "tests:" pipelines_v3/queries/models/transito/schema.yml
   ```

### ValidaÃ§Ã£o Final

Sempre teste a compilaÃ§Ã£o apÃ³s copiar:
```bash
cd pipelines_v3/queries
dbt parse --select transito  # Valida se tudo estÃ¡ correto
```

---

## Estrutura do Projeto Novo

Cada pipeline Ã© um pacote independente com sua prÃ³pria estrutura:
```
pipelines/
â”œâ”€â”€ capture__jae_auxiliar/          # Nome: tipo__fonte
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ CHANGELOG.md
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ constants.py
â”‚   â”œâ”€â”€ flow.py
â”‚   â”œâ”€â”€ prefect.yaml
â”‚   â””â”€â”€ pyproject.toml
â”œâ”€â”€ treatment__cadastro/
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ common/                          # Biblioteca compartilhada
â”‚   â””â”€â”€ ...
â””â”€â”€ templates/                       # Templates do cookiecutter
    â”œâ”€â”€ capture/
    â””â”€â”€ treatment/

queries/                             # Modelos DBT
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ cadastro/                   # Modelos do selector cadastro
â”‚   â”œâ”€â”€ bilhetagem/
â”‚   â””â”€â”€ _sources/                   # DefiniÃ§Ãµes de sources
â”œâ”€â”€ selectors.yml                   # DefiniÃ§Ã£o dos selectors
â”œâ”€â”€ dbt_project.yml
â””â”€â”€ profiles.yml
```

---

## Workflow de MigraÃ§Ã£o

### Passo 0: Atualizar repositÃ³rio de origem
```bash
cd /caminho/para/pipelines_rj_smtr
git checkout main
git pull origin main
```

### Passo 1: Criar estrutura com cookiecutter
```bash
cd /caminho/para/pipelines_v3
uvx cookiecutter templates --output-dir=pipelines
```

Responda as perguntas do template (nome, tipo, etc).

### Passo 2: Verificar o que jÃ¡ existe em common/

Antes de escrever cÃ³digo, cheque:
- Existe task similar em `common/tasks.py`?
- Existe extractor similar em `common/capture/`?
- Existe util similar em `common/utils/`?

### Passo 3: Migrar constants.py

- Remover `Enum`
- Adicionar `tzinfo` nos `datetime()`
- Atualizar imports

### Passo 4: Migrar flow.py

- Usar `@flow` decorator
- ParÃ¢metros viram argumentos da funÃ§Ã£o
- Chamar funÃ§Ã£o genÃ©rica (`create_capture_flows_default_tasks` ou `create_materialization_flows_default_tasks`)

### Passo 5: Criar task especÃ­fica (se necessÃ¡rio)

Se precisar de uma task especÃ­fica (como `create_jae_general_extractor`):
1. Verificar se pode reutilizar algo de `common/`
2. Se for reutilizÃ¡vel, criar em `common/capture/{fonte}/tasks.py`
3. Se for Ãºnico, criar no prÃ³prio pipeline

### Passo 6: [TREATMENT] Copiar modelos DBT

**Apenas para flows de treatment:**
1. Identificar selector no flow
2. Copiar modelos de `queries/models/{dominio}/` **com toda a estrutura** (staging/, schema.yml, CHANGELOG.md)
3. Copiar snapshots de `queries/snapshots/{dominio}/` (se existirem)
4. Verificar se sources estÃ£o definidos em `sources.yml`
5. Verificar se selectors jÃ¡ existem em `selectors.yml`
6. Verificar e copiar dependÃªncias (refs) entre modelos
7. Testar compilaÃ§Ã£o com `dbt parse --select {dominio}`

### Passo 7: Atualizar prefect.yaml

- Definir schedules (cron)
- Configurar deployments staging e prod

### Passo 8: Atualizar CHANGELOG.md

---

## Mapeamento de Conceitos

### 1. DefiniÃ§Ã£o de Flow

**ANTES (1.4):**
```python
from prefect import Parameter, case, unmapped
from prefeitura_rio.pipelines_utils.custom import Flow

with Flow("jae: auxiliares - captura") as CAPTURA_AUXILIAR:
    table_id = Parameter(name="table_id", default="linha")
    timestamp = Parameter(name="timestamp", default=None)
    recapture = Parameter(name="recapture", default=False)

    # tasks...
```

**DEPOIS (3.0):**
```python
from prefect import flow
from pipelines.common.capture.default_capture.utils import rename_capture_flow_run

@flow(log_prints=True, flow_run_name=rename_capture_flow_run)
def capture__jae_auxiliar(
    env=None,
    source_table_ids=tuple([s.table_id for s in sources]),
    timestamp=None,
    recapture=True,
    recapture_days=2,
    recapture_timestamps=None,
):
    # chamada direta de funÃ§Ã£o
    create_capture_flows_default_tasks(...)
```

### 2. Tasks

**ANTES (1.4):**
```python
from prefect import task
from pipelines.constants import constants

@task(
    max_retries=constants.MAX_RETRIES.value,
    retry_delay=timedelta(seconds=constants.RETRY_DELAY.value),
)
def minha_task(param):
    log("mensagem")  # from prefeitura_rio.pipelines_utils.logging import log
    return resultado
```

**DEPOIS (3.0):**
```python
from prefect import task

@task
def minha_task(param):
    print("mensagem")  # usa print() com log_prints=True no flow
    return resultado
```

### 3. Constants

**ANTES (1.4):**
```python
from enum import Enum

class constants(Enum):
    JAE_SOURCE_NAME = "jae"
    JAE_SECRET_PATH = "smtr_jae_access_data"

# Uso: constants.JAE_SOURCE_NAME.value
```

**DEPOIS (3.0):**
```python
# VariÃ¡veis diretas (sem Enum)
JAE_SOURCE_NAME = "jae"
JAE_SECRET_PATH = "smtr_jae_access_data"

# Uso: JAE_SOURCE_NAME
```

### 4. SourceTable

**ANTES (1.4):**
```python
from pipelines.utils.gcp.bigquery import SourceTable
from pipelines.schedules import create_hourly_cron

JAE_AUXILIAR_SOURCES = [
    SourceTable(
        source_name=JAE_SOURCE_NAME,
        table_id=k,
        first_timestamp=datetime(2024, 1, 7, 0, 0, 0),
        schedule_cron=create_hourly_cron(),  # cron explÃ­cito
        primary_keys=v["primary_keys"],
        # ...
    )
]
```

**DEPOIS (3.0):**
```python
from pipelines.common.utils.gcp.bigquery import SourceTable

JAE_AUXILIAR_SOURCES = [
    SourceTable(
        source_name=jae_constants.JAE_SOURCE_NAME,
        table_id=k,
        first_timestamp=v.get(
            "first_timestamp",
            datetime(2024, 1, 7, 0, 0, 0, tzinfo=ZoneInfo(smtr_constants.TIMEZONE)),
        ),
        flow_folder_name="capture__jae_auxiliar",  # infere cron do prefect.yaml
        primary_keys=v["primary_keys"],
        # ...
    )
]
```

### 5. DBTSelector

**ANTES (1.4):**
```python
from pipelines.treatment.templates.utils import DBTSelector
from pipelines.schedules import create_hourly_cron

CADASTRO_SELECTOR = DBTSelector(
    name="cadastro",
    schedule_cron=create_hourly_cron(minute=10),
    initial_datetime=datetime(2025, 3, 26, 0, 0, 0),
)
```

**DEPOIS (3.0):**
```python
from pipelines.common.treatment.default_treatment.utils import DBTSelector

CADASTRO_SELECTOR = DBTSelector(
    name="cadastro",
    initial_datetime=datetime(2025, 3, 26, 0, 0, 0, tzinfo=ZoneInfo(smtr_constants.TIMEZONE)),
    flow_folder_name="treatment__cadastro",  # infere cron do prefect.yaml
)
```

### 6. Runtime e Context

**ANTES (1.4):**
```python
import prefect

flow_name = prefect.context.flow_name
flow_run_id = prefect.context.flow_run_id
```

**DEPOIS (3.0):**
```python
from prefect import runtime

flow_name = runtime.flow_run.flow_name
deployment_name = runtime.deployment.name
scheduled_start_time = runtime.flow_run.scheduled_start_time
task_run_id = runtime.task_run.id
```

### 7. Ambiente (dev/prod)

**ANTES (1.4):**
```python
# Inferido pelo agent label ou variÃ¡vel de ambiente
```

**DEPOIS (3.0):**
```python
from prefect import runtime

def get_run_env(env, deployment_name):
    if deployment_name is not None:
        env = "prod" if deployment_name.endswith("--prod") else "dev"
    # ...
```

### 8. Logging

**ANTES (1.4):**
```python
from prefeitura_rio.pipelines_utils.logging import log
log("mensagem", level="info")
```

**DEPOIS (3.0):**
```python
print("mensagem")  # com log_prints=True no decorator @flow
```

### 9. Secrets

**ANTES (1.4):**
```python
from pipelines.utils.secret import get_secret
credentials = get_secret(constants.JAE_SECRET_PATH.value)
```

**DEPOIS (3.0):**
```python
from pipelines.common.utils.secret import get_secret
credentials = get_secret(jae_constants.JAE_SECRET_PATH)  # sem .value
```

---

## PadrÃµes de Nomenclatura

| Tipo | PadrÃ£o Antigo | PadrÃ£o Novo |
|------|---------------|-------------|
| Pasta do flow | `capture/jae/` | `capture__jae_auxiliar/` |
| Nome do flow | `jae: auxiliares - captura` | `capture__jae_auxiliar` |
| FunÃ§Ã£o do flow | `CAPTURA_AUXILIAR` (variÃ¡vel) | `capture__jae_auxiliar` (funÃ§Ã£o) |
| Deployment staging | N/A | `rj-capture--jae_auxiliar--staging` |
| Deployment prod | N/A | `rj-capture--jae_auxiliar--prod` |

---

## Checklist de MigraÃ§Ã£o

Para cada flow:

- [ ] **Atualizar repo origem:** `git checkout main && git pull origin main`
- [ ] Criar estrutura com `uvx cookiecutter templates --output-dir=pipelines`
- [ ] Verificar `common/` para reutilizaÃ§Ã£o de cÃ³digo
- [ ] Migrar `constants.py` (remover Enum, usar variÃ¡veis diretas)
- [ ] Migrar `flow.py`:
  - [ ] Substituir `with Flow()` por `@flow`
  - [ ] Substituir `Parameter()` por argumentos de funÃ§Ã£o
  - [ ] Usar `runtime` ao invÃ©s de `prefect.context`
  - [ ] Substituir `log()` por `print()`
- [ ] Criar tasks especÃ­ficas em `common/` se reutilizÃ¡veis
- [ ] Atualizar imports para `pipelines.common.*`
- [ ] Adicionar `tzinfo` em todos os `datetime()`
- [ ] Remover `.value` das constants
- [ ] **[TREATMENT]** Copiar modelos DBT para `queries/models/` (inclui staging/, schema.yml, CHANGELOG.md)
- [ ] **[TREATMENT]** Copiar snapshots para `queries/snapshots/{dominio}/` (se existirem)
- [ ] **[TREATMENT]** Verificar se sources estÃ£o definidos em `queries/models/sources.yml`
- [ ] **[TREATMENT]** Verificar se selectors existem em `queries/selectors.yml`
- [ ] **[TREATMENT]** Verificar dependÃªncias (refs) entre modelos e copiar modelos dependentes se necessÃ¡rio
- [ ] **[TREATMENT]** Testar compilaÃ§Ã£o com `dbt parse --select {dominio}`
- [ ] Configurar `prefect.yaml` com schedules
- [ ] Atualizar `CHANGELOG.md`
- [ ] Testar localmente

---

## Imports Comuns
```python
# Flow e runtime
from prefect import flow, runtime, unmapped
from prefect.tasks import Task

# Tasks comuns (VERIFICAR ANTES DE CRIAR NOVAS)
from pipelines.common.tasks import (
    get_run_env,
    get_scheduled_timestamp,
    setup_environment,
)

# Captura
from pipelines.common.capture.default_capture.flow import create_capture_flows_default_tasks
from pipelines.common.capture.default_capture.utils import rename_capture_flow_run

# Tratamento
from pipelines.common.treatment.default_treatment.flow import create_materialization_flows_default_tasks
from pipelines.common.treatment.default_treatment.utils import rename_treatment_flow_run, DBTSelector

# Utils (VERIFICAR ANTES DE CRIAR NOVAS)
from pipelines.common.utils.gcp.bigquery import SourceTable
from pipelines.common.utils.secret import get_secret
from pipelines.common import constants as smtr_constants
```
